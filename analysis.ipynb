{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model \n",
    "import pickle\n",
    "import torch as tc\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(tensor, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    plt.imshow(tensor.detach().numpy(), cmap='RdBu', **kwargs)\n",
    "    plt.xlabel(xaxis)\n",
    "    plt.ylabel(yaxis)\n",
    "    plt.show()\n",
    "\n",
    "timestamp = '20240429143845'\n",
    "with open(f'saved_models/{timestamp}.cfg', 'rb') as f:\n",
    "    cfg = pickle.load(f)\n",
    "# load model\n",
    "model = HookedTransformer(cfg)\n",
    "model.load_state_dict(tc.load(f'saved_models/{timestamp}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 78125 sequences of length 7 with vocabulary size 5\n",
      "Stacked sequences shape: torch.Size([78125, 7])\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "vocab_size = cfg.d_vocab_out\n",
    "input_size = cfg.d_vocab\n",
    "def get_all_sequences(max_seq_len, vocab_size):\n",
    "    # Generate all possible sequences of length n with vocabulary size m\n",
    "    sequences = itertools.product(range(vocab_size), repeat=max_seq_len)\n",
    "    sequences = [tc.tensor(seq) for seq in sequences]\n",
    "    return sequences\n",
    "sequences = get_all_sequences(input_size, vocab_size)\n",
    "print(f'Generated {len(sequences)} sequences of length {input_size} with vocabulary size {vocab_size}')\n",
    "sequences = tc.stack(sequences)\n",
    "print(f'Stacked sequences shape: {sequences.shape}')\n",
    "\n",
    "original_logits, cache = model.run_with_cache(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed torch.Size([78125, 7, 34])\n",
      "hook_pos_embed torch.Size([78125, 7, 34])\n",
      "blocks.0.hook_resid_pre torch.Size([78125, 7, 34])\n",
      "blocks.0.attn.hook_q torch.Size([78125, 7, 1, 9])\n",
      "blocks.0.attn.hook_k torch.Size([78125, 7, 1, 9])\n",
      "blocks.0.attn.hook_v torch.Size([78125, 7, 1, 9])\n",
      "blocks.0.attn.hook_attn_scores torch.Size([78125, 1, 7, 7])\n",
      "blocks.0.attn.hook_pattern torch.Size([78125, 1, 7, 7])\n",
      "blocks.0.attn.hook_z torch.Size([78125, 7, 1, 9])\n",
      "blocks.0.hook_attn_out torch.Size([78125, 7, 34])\n",
      "blocks.0.hook_resid_mid torch.Size([78125, 7, 34])\n",
      "blocks.0.mlp.hook_pre torch.Size([78125, 7, 35])\n",
      "blocks.0.mlp.hook_post torch.Size([78125, 7, 35])\n",
      "blocks.0.hook_mlp_out torch.Size([78125, 7, 34])\n",
      "blocks.0.hook_resid_post torch.Size([78125, 7, 34])\n",
      "blocks.1.hook_resid_pre torch.Size([78125, 7, 34])\n",
      "blocks.1.attn.hook_q torch.Size([78125, 7, 1, 9])\n",
      "blocks.1.attn.hook_k torch.Size([78125, 7, 1, 9])\n",
      "blocks.1.attn.hook_v torch.Size([78125, 7, 1, 9])\n",
      "blocks.1.attn.hook_attn_scores torch.Size([78125, 1, 7, 7])\n",
      "blocks.1.attn.hook_pattern torch.Size([78125, 1, 7, 7])\n",
      "blocks.1.attn.hook_z torch.Size([78125, 7, 1, 9])\n",
      "blocks.1.hook_attn_out torch.Size([78125, 7, 34])\n",
      "blocks.1.hook_resid_mid torch.Size([78125, 7, 34])\n",
      "blocks.1.mlp.hook_pre torch.Size([78125, 7, 35])\n",
      "blocks.1.mlp.hook_post torch.Size([78125, 7, 35])\n",
      "blocks.1.hook_mlp_out torch.Size([78125, 7, 34])\n",
      "blocks.1.hook_resid_post torch.Size([78125, 7, 34])\n",
      "blocks.2.hook_resid_pre torch.Size([78125, 7, 34])\n",
      "blocks.2.attn.hook_q torch.Size([78125, 7, 1, 9])\n",
      "blocks.2.attn.hook_k torch.Size([78125, 7, 1, 9])\n",
      "blocks.2.attn.hook_v torch.Size([78125, 7, 1, 9])\n",
      "blocks.2.attn.hook_attn_scores torch.Size([78125, 1, 7, 7])\n",
      "blocks.2.attn.hook_pattern torch.Size([78125, 1, 7, 7])\n",
      "blocks.2.attn.hook_z torch.Size([78125, 7, 1, 9])\n",
      "blocks.2.hook_attn_out torch.Size([78125, 7, 34])\n",
      "blocks.2.hook_resid_mid torch.Size([78125, 7, 34])\n",
      "blocks.2.mlp.hook_pre torch.Size([78125, 7, 35])\n",
      "blocks.2.mlp.hook_post torch.Size([78125, 7, 35])\n",
      "blocks.2.hook_mlp_out torch.Size([78125, 7, 34])\n",
      "blocks.2.hook_resid_post torch.Size([78125, 7, 34])\n"
     ]
    }
   ],
   "source": [
    "for param_name, param in cache.items():\n",
    "    print(param_name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1dffe1ae15a4d0fa7f4c05a332529bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, description='i:', max=78124)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c4a29c1b4e4a09b08c66f4d13477aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i:', max=78124), Output()), _dom_classes=('widget-interaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a slider to control the value of i\n",
    "slider = widgets.IntSlider(min=0, max=len(sequences)-1, value=0, description='i:')\n",
    "#display(slider)\n",
    "\n",
    "# Function to update the outputs based on the value of i\n",
    "def update_outputs(i):\n",
    "    print(sequences[i])\n",
    "    #plt.gcf().clear()  # Clear the current figure\n",
    "    # use cmap='RdBu' for better visualisation of negative values\n",
    "    plt.set_cmap('RdBu')\n",
    "    \n",
    "    # plt.imshow(cache[\"hook_embed\"][i])\n",
    "    # plt.colorbar(shrink=0.3)\n",
    "    # plt.title(\"hook_embed\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # plt.imshow(cache[\"hook_pos_embed\"][i])\n",
    "    # plt.colorbar(shrink=0.3)\n",
    "    # plt.title(\"hook_pos_embed\")\n",
    "    # plt.show()\n",
    "    plt.imshow(cache[\"embedding: blocks.0.hook_resid_pre\"][i])\n",
    "    plt.colorbar(shrink=0.3)\n",
    "    plt.title(\"blocks.0.hook_resid_pre\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(cache[\"blocks.0.hook_attn_out\"][i])\n",
    "    plt.colorbar(shrink=0.3)\n",
    "    plt.title(\"blocks.0.hook_attn_out\")\n",
    "    plt.show()\n",
    "\n",
    "    imshow(cache[\"pre\", 0, \"attn\"][i])\n",
    "\n",
    "    # imshow(cache[\"pre\", 0, \"mlp\"][i])\n",
    "\n",
    "    # imshow(cache[\"pre\", 1, \"attn\"][i])\n",
    "\n",
    "    # imshow(cache[\"pre\", 1, \"mlp\"][i])\n",
    "    # imshow(cache[\"pre\", 2, \"attn\"][i])\n",
    "\n",
    "    # imshow(cache[\"pre\", 2, \"mlp\"][i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Connect the slider to the update_outputs function\n",
    "# slider.observe(lambda change: update_outputs(change.new), names='value')\n",
    "\n",
    "\n",
    "interactive_output = interactive(update_outputs, i=slider)\n",
    "display(slider, interactive_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO reverse engineer how the model works, start with the first layer and then check what each key does and how they relate to each other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sortinterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
